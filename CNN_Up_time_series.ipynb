{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLg7kZLhdysh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dow_jones = pd.read_csv('/content/Processed_DJI.csv')\n",
        "Nasdaq = pd.read_csv('/content/Processed_NASDAQ.csv')\n",
        "Nyse = pd.read_csv('/content/Processed_NYSE.csv')\n",
        "Russell = pd.read_csv('/content/Processed_RUSSELL.csv')\n",
        "S_P = pd.read_csv('/content/Processed_S&P.csv')"
      ],
      "metadata": {
        "id": "GSEGvQ10e6zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Dow_jones.shape)\n",
        "print(Nasdaq.shape)\n",
        "print(Nyse.shape)\n",
        "print(Russell.shape)\n",
        "print(S_P.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVgp39nxgGvl",
        "outputId": "d0cef04d-f8f7-4295-9437-7586bef9ce31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1984, 84)\n",
            "(1984, 84)\n",
            "(1984, 84)\n",
            "(1984, 84)\n",
            "(1984, 84)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Dow_jones.isnull().sum().sum())\n",
        "print(Nasdaq.isnull().sum().sum())\n",
        "print(Nyse.isnull().sum().sum())\n",
        "print(Russell.isnull().sum().sum())\n",
        "print(S_P.isnull().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU9VgTecgJUY",
        "outputId": "8ac25a80-5358-47b8-b6e4-45492d3888e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3031\n",
            "3029\n",
            "3029\n",
            "3029\n",
            "3029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Dow_jones.fillna(Dow_jones.median(),inplace = True)\n",
        "Nasdaq.fillna(Nasdaq.median(),inplace = True)\n",
        "Nyse.fillna(Nyse.median(),inplace = True)\n",
        "Russell.fillna(Russell.median(),inplace = True)\n",
        "S_P.fillna(S_P.median(),inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TBxr4XHgiHc",
        "outputId": "18f6d9c9-6f79-4b5f-8030-1d72ee61f553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Dow_jones.isnull().sum().sum())\n",
        "print(Nasdaq.isnull().sum().sum())\n",
        "print(Nyse.isnull().sum().sum())\n",
        "print(Russell.isnull().sum().sum())\n",
        "print(S_P.isnull().sum().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlnQ6WmkhOl-",
        "outputId": "5684d181-3d6b-4664-a0a0-d9d260d9cd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S_P.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWkM5H4KhQRE",
        "outputId": "79caf85a-ea11-436a-b6be-d02e040b0b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1984 entries, 0 to 1983\n",
            "Data columns (total 84 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Date            1984 non-null   object \n",
            " 1   Close           1984 non-null   float64\n",
            " 2   Volume          1984 non-null   float64\n",
            " 3   mom             1984 non-null   float64\n",
            " 4   mom1            1984 non-null   float64\n",
            " 5   mom2            1984 non-null   float64\n",
            " 6   mom3            1984 non-null   float64\n",
            " 7   ROC_5           1984 non-null   float64\n",
            " 8   ROC_10          1984 non-null   float64\n",
            " 9   ROC_15          1984 non-null   float64\n",
            " 10  ROC_20          1984 non-null   float64\n",
            " 11  EMA_10          1984 non-null   float64\n",
            " 12  EMA_20          1984 non-null   float64\n",
            " 13  EMA_50          1984 non-null   float64\n",
            " 14  EMA_200         1984 non-null   float64\n",
            " 15  DTB4WK          1984 non-null   float64\n",
            " 16  DTB3            1984 non-null   float64\n",
            " 17  DTB6            1984 non-null   float64\n",
            " 18  DGS5            1984 non-null   float64\n",
            " 19  DGS10           1984 non-null   float64\n",
            " 20  Oil             1984 non-null   float64\n",
            " 21  Gold            1984 non-null   float64\n",
            " 22  DAAA            1984 non-null   float64\n",
            " 23  DBAA            1984 non-null   float64\n",
            " 24  GBP             1984 non-null   float64\n",
            " 25  JPY             1984 non-null   float64\n",
            " 26  CAD             1984 non-null   float64\n",
            " 27  CNY             1984 non-null   float64\n",
            " 28  AAPL            1984 non-null   float64\n",
            " 29  AMZN            1984 non-null   float64\n",
            " 30  GE              1984 non-null   float64\n",
            " 31  JNJ             1984 non-null   float64\n",
            " 32  JPM             1984 non-null   float64\n",
            " 33  MSFT            1984 non-null   float64\n",
            " 34  WFC             1984 non-null   float64\n",
            " 35  XOM             1984 non-null   float64\n",
            " 36  FCHI            1984 non-null   float64\n",
            " 37  FTSE            1984 non-null   float64\n",
            " 38  GDAXI           1984 non-null   float64\n",
            " 39  DJI             1984 non-null   float64\n",
            " 40  HSI             1984 non-null   float64\n",
            " 41  IXIC            1984 non-null   float64\n",
            " 42  SSEC            1984 non-null   float64\n",
            " 43  RUT             1984 non-null   float64\n",
            " 44  NYSE            1984 non-null   float64\n",
            " 45  TE1             1984 non-null   float64\n",
            " 46  TE2             1984 non-null   float64\n",
            " 47  TE3             1984 non-null   float64\n",
            " 48  TE5             1984 non-null   float64\n",
            " 49  TE6             1984 non-null   float64\n",
            " 50  DE1             1984 non-null   float64\n",
            " 51  DE2             1984 non-null   float64\n",
            " 52  DE4             1984 non-null   float64\n",
            " 53  DE5             1984 non-null   float64\n",
            " 54  DE6             1984 non-null   float64\n",
            " 55  CTB3M           1984 non-null   float64\n",
            " 56  CTB6M           1984 non-null   float64\n",
            " 57  CTB1Y           1984 non-null   float64\n",
            " 58  Name            1984 non-null   object \n",
            " 59  AUD             1984 non-null   float64\n",
            " 60  Brent           1984 non-null   float64\n",
            " 61  CAC-F           1984 non-null   float64\n",
            " 62  copper-F        1984 non-null   float64\n",
            " 63  WIT-oil         1984 non-null   float64\n",
            " 64  DAX-F           1984 non-null   float64\n",
            " 65  DJI-F           1984 non-null   float64\n",
            " 66  EUR             1984 non-null   float64\n",
            " 67  FTSE-F          1984 non-null   float64\n",
            " 68  gold-F          1984 non-null   float64\n",
            " 69  HSI-F           1984 non-null   float64\n",
            " 70  KOSPI-F         1984 non-null   float64\n",
            " 71  NASDAQ-F        1984 non-null   float64\n",
            " 72  GAS-F           1984 non-null   float64\n",
            " 73  Nikkei-F        1984 non-null   float64\n",
            " 74  NZD             1984 non-null   float64\n",
            " 75  silver-F        1984 non-null   float64\n",
            " 76  RUSSELL-F       1984 non-null   float64\n",
            " 77  S&P-F           1984 non-null   float64\n",
            " 78  CHF             1984 non-null   float64\n",
            " 79  Dollar index-F  1984 non-null   float64\n",
            " 80  Dollar index    1984 non-null   float64\n",
            " 81  wheat-F         1984 non-null   float64\n",
            " 82  XAG             1984 non-null   float64\n",
            " 83  XAU             1984 non-null   float64\n",
            "dtypes: float64(82), object(2)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I will prepare the data as it should be like the paper said\n",
        "Dow_jones['Day'] = pd.to_datetime(Dow_jones['Date']).dt.weekday\n",
        "Nasdaq['Day'] = pd.to_datetime(Nasdaq['Date']).dt.weekday\n",
        "Nyse['Day'] = pd.to_datetime(Nyse['Date']).dt.weekday\n",
        "Russell['Day'] = pd.to_datetime(Russell['Date']).dt.weekday\n",
        "S_P['Day'] = pd.to_datetime(S_P['Date']).dt.weekday"
      ],
      "metadata": {
        "id": "SBe2J7E3hTVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The paper said they have 82 features and We have here 84 so it means that have two more features\n",
        "# The first one is the Name and the second is mom and now as I added additional varaible which is Day\n",
        "# now We have to drop Date or put it as it's our index\n",
        "Dow_jones.index = pd.to_datetime(Dow_jones['Date'], format='%Y.%m.%d')\n",
        "Nasdaq.index = pd.to_datetime(Nasdaq['Date'], format='%Y.%m.%d')\n",
        "Nyse.index = pd.to_datetime(Nyse['Date'], format='%Y.%m.%d')\n",
        "Russell.index = pd.to_datetime(Russell['Date'], format='%Y.%m.%d')\n",
        "S_P.index = pd.to_datetime(S_P['Date'], format='%Y.%m.%d')"
      ],
      "metadata": {
        "id": "SCal3Rb0jJ5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dow_jones.drop(['Date','Name','mom'],axis=1,inplace=True)\n",
        "Nasdaq.drop(['Date','Name','mom'],axis=1,inplace=True)\n",
        "Nyse.drop(['Date','Name','mom'],axis=1,inplace=True)\n",
        "Russell.drop(['Date','Name','mom'],axis=1,inplace=True)\n",
        "S_P.drop(['Date','Name','mom'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "QvNkDuKtjdnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now We check that We have the same features as the paper \n",
        "print(Dow_jones.shape)\n",
        "print(Nasdaq.shape)\n",
        "print(Nyse.shape)\n",
        "print(Russell.shape)\n",
        "print(S_P.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnlUqTJOkArv",
        "outputId": "56261085-9552-4985-d2ab-393e92885d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1984, 82)\n",
            "(1984, 82)\n",
            "(1984, 82)\n",
            "(1984, 82)\n",
            "(1984, 82)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to normalize the data\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "CVEAWBMDnd6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "std_scaler =  StandardScaler()"
      ],
      "metadata": {
        "id": "maaNzJ47qF45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# But before making normalization We need to split the data\n",
        "def split_data(df):\n",
        "  \n",
        "  train_df = df[:1191]\n",
        "  val_df = df[1191:1587]\n",
        "  test_df = df[1587:]\n",
        "\n",
        "  return train_df,val_df,test_df"
      ],
      "metadata": {
        "id": "cJgVItd5qmTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_names = list(Dow_jones.columns)\n",
        "columns_names.remove('Day')"
      ],
      "metadata": {
        "id": "Wh2X2RNhucue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So I will start by using Dow Jones\n",
        "Dow_jones_train, Dow_jones_val, Dow_jones_test = split_data(Dow_jones)"
      ],
      "metadata": {
        "id": "7be-dI-2q2bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# So I make standardization for all the features but for not Day feature\n",
        "Dowjones_scaler_train = pd.DataFrame(std_scaler.fit_transform(Dow_jones_train.iloc[:,:-1]),columns=columns_names)"
      ],
      "metadata": {
        "id": "ft4EgGlprXjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dowjones_scaler_val = pd.DataFrame(std_scaler.transform(Dow_jones_val.iloc[:,:-1]),columns=columns_names)\n",
        "Dowjones_scaler_test = pd.DataFrame(std_scaler.transform(Dow_jones_test.iloc[:,:-1]),columns=columns_names)"
      ],
      "metadata": {
        "id": "lsy7kdHgtGGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dowjones_scaler_train['Day'] = Dow_jones['Day'][:1191]\n",
        "Dowjones_scaler_val['Day'] = Dow_jones['Day'][1191:1587]\n",
        "Dowjones_scaler_test['Day'] = Dow_jones['Day'][1587:]"
      ],
      "metadata": {
        "id": "lkcOWzrIsqpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now We have done preparation as the paper said\n",
        "# Now I will try to implement the model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential  "
      ],
      "metadata": {
        "id": "LkQUQAvwuQpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()"
      ],
      "metadata": {
        "id": "8wAElNtzvKWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Conv2D(8,(1,82),activation='relu'))"
      ],
      "metadata": {
        "id": "l3JIRqzBwzhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(S_P[\"Close\"].pct_change().shift(-1) > 0).astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vQXjiPRDx-d",
        "outputId": "fa004a06-4e90-44f4-ba2f-a365ac35458f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2009-12-31    1\n",
              "2010-01-04    1\n",
              "2010-01-05    1\n",
              "2010-01-06    1\n",
              "2010-01-07    1\n",
              "             ..\n",
              "2017-11-09    0\n",
              "2017-11-10    1\n",
              "2017-11-13    0\n",
              "2017-11-14    0\n",
              "2017-11-15    0\n",
              "Name: Close, Length: 1984, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S_P[\"Close\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKwPPaxCFJdd",
        "outputId": "70db6ae1-f143-4da5-e05f-f489be46ba6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date\n",
              "2009-12-31    1115.099976\n",
              "2010-01-04    1132.989990\n",
              "2010-01-05    1136.520020\n",
              "2010-01-06    1137.140015\n",
              "2010-01-07    1141.689941\n",
              "                 ...     \n",
              "2017-11-09    2584.620117\n",
              "2017-11-10    2582.300049\n",
              "2017-11-13    2584.840088\n",
              "2017-11-14    2578.870117\n",
              "2017-11-15    2564.620117\n",
              "Name: Close, Length: 1984, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "eojFkjDRkd1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error,mean_squared_error\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def cnnpred_2d(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Conv2D(n_filters[2], kernel_size=(3,1), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(2,1)),\n",
        "        Flatten(),\n",
        "        #Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)     # pick one time step\n",
        "            n = (df.index == t).argmax() # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []\n",
        "\n",
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        # extract sample using a sliding window\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 8\n",
        "n_features = 82\n",
        "\n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_2d(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        "\n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfwu7jgJFL74",
        "outputId": "c1f5d9f8-5bb6-41d0-b82b-5a9e41c7e8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 60, 1, 8)          664       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 58, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 29, 1, 8)         0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 27, 1, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 1, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 104)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,169\n",
            "Trainable params: 1,169\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 68s 139ms/step - loss: 0.4453 - acc: 0.5580 - f1macro: 0.3589 - val_loss: 0.4944 - val_acc: 0.5055 - val_f1macro: 0.3353\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 55s 139ms/step - loss: 0.4075 - acc: 0.5980 - f1macro: 0.4509 - val_loss: 0.4770 - val_acc: 0.5172 - val_f1macro: 0.3877\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 57s 142ms/step - loss: 0.2942 - acc: 0.7191 - f1macro: 0.6736 - val_loss: 0.4660 - val_acc: 0.5367 - val_f1macro: 0.4527\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 58s 146ms/step - loss: 0.2358 - acc: 0.7703 - f1macro: 0.7443 - val_loss: 0.4938 - val_acc: 0.5016 - val_f1macro: 0.4500\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 57s 142ms/step - loss: 0.2236 - acc: 0.7796 - f1macro: 0.7568 - val_loss: 0.5199 - val_acc: 0.4836 - val_f1macro: 0.4551\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 54s 137ms/step - loss: 0.2139 - acc: 0.7886 - f1macro: 0.7688 - val_loss: 0.4927 - val_acc: 0.5133 - val_f1macro: 0.4625\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 55s 137ms/step - loss: 0.2055 - acc: 0.7960 - f1macro: 0.7768 - val_loss: 0.4891 - val_acc: 0.5094 - val_f1macro: 0.4673\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 55s 138ms/step - loss: 0.2026 - acc: 0.7991 - f1macro: 0.7814 - val_loss: 0.4755 - val_acc: 0.5305 - val_f1macro: 0.4941\n",
            "accuracy: 0.5336585365853659\n",
            "MAE: 0.46634146341463417\n",
            "F1: 0.5828970331588134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def cnnpred_3d(seq_len=60, n_stocks=5, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"3D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_stocks, seq_len, n_features)),\n",
        "        Conv2D(n_filters[0], kernel_size=(1,1), activation=\"relu\", data_format=\"channels_last\"),\n",
        "        Conv2D(n_filters[1], kernel_size=(n_stocks,3), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(1,2)),\n",
        "        Conv2D(n_filters[2], kernel_size=(1,3), activation=\"relu\"),\n",
        "        MaxPool2D(pool_size=(1,2)),\n",
        "        Flatten(),\n",
        "        Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def datagen(data, seq_len, batch_size, target_index, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    # Learn about the data's features and time axis\n",
        "    input_cols = [c for c in data.columns if c[0] != targetcol]\n",
        "    tickers = sorted(set(c for _,c in input_cols))\n",
        "    n_features = len(input_cols) // len(tickers)\n",
        "    index = data.index[data.index < TRAIN_TEST_CUTOFF]\n",
        "    split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "    assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "    if kind == \"train\":\n",
        "        index = index[:split]   # range for the training set\n",
        "    elif kind == 'valid':\n",
        "        index = index[split:]   # range for the validation set\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    # Infinite loop to generate a batch\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)\n",
        "            n = (data.index == t).argmax()\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = data.iloc[n-seq_len+1:n+1][input_cols]\n",
        "            # convert frame with two level of indices into 3D array\n",
        "            shape = (len(tickers), len(frame), n_features)\n",
        "            X = np.full(shape, np.nan)\n",
        "            for i,ticker in enumerate(tickers):\n",
        "                X[i] = frame.xs(ticker, axis=1, level=1).values\n",
        "            batch.append([X, data[targetcol][target_index][t]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            yield np.array(X), np.array(y)\n",
        "            batch = []\n",
        "\n",
        "def testgen(data, seq_len, target_index, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    input_cols = [c for c in data.columns if c[0] != targetcol]\n",
        "    tickers = sorted(set(c for _,c in input_cols))\n",
        "    n_features = len(input_cols) // len(tickers)\n",
        "    t = data.index[data.index >= TRAIN_TEST_CUTOFF][0]\n",
        "    n = (data.index == t).argmax()\n",
        "    batch = []\n",
        "    for i in range(n+1, len(data)+1):\n",
        "        # Clip a window of seq_len ends at row position i-1\n",
        "        frame = data.iloc[i-seq_len:i]\n",
        "        target = frame[targetcol][target_index][-1]\n",
        "        frame = frame[input_cols]\n",
        "        # convert frame with two level of indices into 3D array\n",
        "        shape = (len(tickers), len(frame), n_features)\n",
        "        X = np.full(shape, np.nan)\n",
        "        for i,ticker in enumerate(tickers):\n",
        "            X[i] = frame.xs(ticker, axis=1, level=1).values\n",
        "        batch.append([X, target])\n",
        "    X, y = zip(*batch)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir(DATADIR):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join(DATADIR, filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "# Transform data into 3D dataframe (multilevel columns)\n",
        "for key, df in data.items():\n",
        "    df.columns = pd.MultiIndex.from_product([df.columns, [key]])\n",
        "data = pd.concat(data.values(), axis=1)\n",
        "\n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 5\n",
        "n_features = 82\n",
        "n_stocks = 5\n",
        "\n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = cnnpred_3d(seq_len, n_stocks, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary() # print model structure to console\n",
        "\n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp3d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "\n",
        "model.fit(datagen(data, seq_len, batch_size, \"DJI\", \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"DJI\", \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"DJI\", \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5-mGClfPcTq",
        "outputId": "6aec0efd-b4da-4d36-d0c8-6ad1bd86daa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 5, 60, 8)          664       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 1, 58, 8)          968       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 29, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 1, 27, 8)          200       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 13, 8)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 104)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 104)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 105       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,937\n",
            "Trainable params: 1,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "400/400 [==============================] - 367s 918ms/step - loss: 0.3314 - acc: 0.7059 - f1macro: 0.6728 - val_loss: 0.5129 - val_acc: 0.4734 - val_f1macro: 0.4226\n",
            "Epoch 2/5\n",
            "400/400 [==============================] - 366s 918ms/step - loss: 0.1779 - acc: 0.8475 - f1macro: 0.8418 - val_loss: 0.5301 - val_acc: 0.4602 - val_f1macro: 0.4431\n",
            "Epoch 3/5\n",
            "400/400 [==============================] - 360s 901ms/step - loss: 0.1268 - acc: 0.8904 - f1macro: 0.8870 - val_loss: 0.5071 - val_acc: 0.4961 - val_f1macro: 0.4771\n",
            "Epoch 4/5\n",
            "400/400 [==============================] - 376s 941ms/step - loss: 0.1047 - acc: 0.9071 - f1macro: 0.9043 - val_loss: 0.5044 - val_acc: 0.4898 - val_f1macro: 0.4750\n",
            "Epoch 5/5\n",
            "400/400 [==============================] - 365s 915ms/step - loss: 0.0920 - acc: 0.9170 - f1macro: 0.9146 - val_loss: 0.4941 - val_acc: 0.5008 - val_f1macro: 0.4822\n",
            "accuracy: 0.5414634146341464\n",
            "MAE: 0.4585365853658537\n",
            "F1: 0.6356589147286822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "WCS-z0jg0wC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def lstm_model(seq_len=60, n_features=82):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len,n_features)),\n",
        "        #Conv2D(8, kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        #MaxPool2D(pool_size=(2,1)),\n",
        "        #Flatten(),\n",
        "        LSTM(32, activation=\"relu\",return_sequences=True),\n",
        "        Dropout(0.4),\n",
        "        LSTM(16, activation=\"relu\"),\n",
        "        Dropout(0.4),\n",
        "        Dense(16,activation=\"relu\"),\n",
        "        #Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "\n",
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)     # pick one time step\n",
        "            n = (df.index == t).argmax() # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X.reshape((X.shape[0],X.shape[1],82)), y.astype(np.float)\n",
        "            batch = []\n",
        "\n",
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        # extract sample using a sliding window\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    #return np.expand_dims(np.array(X),3), np.array(y)\n",
        "    X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "    #print(X.shape)\n",
        "    return X.reshape((X.shape[0],X.shape[1],82)), y.astype(np.float)\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 5\n",
        "n_features = 82\n",
        "\n",
        "# Produce Lstm model as a binary classification problem\n",
        "model = lstm_model(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=root_mean_squared_error, metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        "\n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"RMSE:\", mean_squared_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi1_OoP-TFBB",
        "outputId": "02f0faaa-582f-4035-dbf9-de429d4f8265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 60, 32)            14720     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 60, 32)            0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,145\n",
            "Trainable params: 18,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400/400 [==============================] - 156s 384ms/step - loss: 0.4816 - acc: 0.5971 - f1macro: 0.5089 - val_loss: 0.5207 - val_acc: 0.5039 - val_f1macro: 0.4731\n",
            "Epoch 2/5\n",
            "400/400 [==============================] - 150s 375ms/step - loss: 0.4246 - acc: 0.7247 - f1macro: 0.7149 - val_loss: 0.5956 - val_acc: 0.5477 - val_f1macro: 0.5379\n",
            "Epoch 3/5\n",
            "400/400 [==============================] - 146s 364ms/step - loss: 0.3464 - acc: 0.8369 - f1macro: 0.8328 - val_loss: 0.6176 - val_acc: 0.5477 - val_f1macro: 0.5440\n",
            "Epoch 4/5\n",
            "400/400 [==============================] - 150s 375ms/step - loss: 0.2948 - acc: 0.8901 - f1macro: 0.8873 - val_loss: 0.6223 - val_acc: 0.5469 - val_f1macro: 0.5439\n",
            "Epoch 5/5\n",
            "400/400 [==============================] - 149s 372ms/step - loss: 0.2695 - acc: 0.9100 - f1macro: 0.9078 - val_loss: 0.6412 - val_acc: 0.5320 - val_f1macro: 0.5314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.4946341463414634\n",
            "RMSE: 0.5053658536585366\n",
            "F1: 0.5795454545454545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def lstm_model(seq_len=60, n_features=82):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len,n_features)),\n",
        "        #Conv2D(8, kernel_size=(1, n_features), activation=\"relu\"),\n",
        "        #MaxPool2D(pool_size=(2,1)),\n",
        "        #Flatten(),\n",
        "        LSTM(32, activation=\"relu\",return_sequences=True),\n",
        "        Dropout(0.4),\n",
        "        LSTM(16, activation=\"relu\"),\n",
        "        Dropout(0.4),\n",
        "        Dense(16,activation=\"relu\"),\n",
        "        #Dropout(droprate),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
        "\n",
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)     # pick one time step\n",
        "            n = (df.index == t).argmax() # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X.reshape((X.shape[0],X.shape[1],82)), y.astype(np.float)\n",
        "            batch = []\n",
        "\n",
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        # extract sample using a sliding window\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    #return np.expand_dims(np.array(X),3), np.array(y)\n",
        "    X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "    #print(X.shape)\n",
        "    return X.reshape((X.shape[0],X.shape[1],82)), y.astype(np.float)\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "seq_len = 20\n",
        "batch_size = 128\n",
        "n_epochs = 5\n",
        "n_features = 82\n",
        "\n",
        "# Produce lstm_pred as a binary classification problem\n",
        "model = lstm_model(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=root_mean_squared_error, metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        "\n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\n",
        "\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"RMSE:\", mean_squared_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "id": "M9QWs19HvSAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "e8242e60-c0d5-4e69-f723-4ca3850059f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 20, 32)            14720     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 20, 32)            0         \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 16)                3136      \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 16)                272       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,145\n",
            "Trainable params: 18,145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400/400 [==============================] - 90s 217ms/step - loss: 0.4837 - acc: 0.5946 - f1macro: 0.5052 - val_loss: 0.5460 - val_acc: 0.5625 - val_f1macro: 0.5523\n",
            "Epoch 2/5\n",
            "400/400 [==============================] - 87s 217ms/step - loss: 0.4242 - acc: 0.7278 - f1macro: 0.7187 - val_loss: 0.6127 - val_acc: 0.5445 - val_f1macro: 0.5424\n",
            "Epoch 3/5\n",
            "176/400 [============>.................] - ETA: 48s - loss: 0.3809 - acc: 0.7941 - f1macro: 0.7885"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9fae1b7cf625>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n\u001b[1;32m    145\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Target\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m           epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Prepare test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.ar_model import AutoReg"
      ],
      "metadata": {
        "id": "eyXVnAcl1qDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Input\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_absolute_error,mean_squared_error"
      ],
      "metadata": {
        "id": "Pu9pBw0XVViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import plot_importance, plot_tree\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    train = df.iloc[:split,:]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    split = int(len(index)*TRAIN_VALID_RATIO)\n",
        "\n",
        "    X_train = train.iloc[:split,:-1]\n",
        "    y_train = train.iloc[:split,-1]\n",
        "\n",
        "    X_val = train.iloc[split:,:-1]\n",
        "    y_val = train.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_val,y_val,X_test,y_test = datagen(data,'DJI')\n",
        "reg = xgb.XGBRegressor(n_estimators=1000)\n",
        "reg.fit(X_train, y_train,\n",
        "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "        early_stopping_rounds=50,\n",
        "       verbose=False)\n",
        "\n",
        "# Test the model\n",
        "test_out = reg.predict(X_test)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUQjU220VdzP",
        "outputId": "0e89026e-8929-4801-94ff-17a5e9318247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[09:06:56] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "accuracy: 0.5073170731707317\n",
            "RMSE: 0.7019137602506936\n",
            "F1: 0.61003861003861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from xgboost import plot_importance, plot_tree\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    train = df.iloc[:split,:]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    split = int(len(index)*TRAIN_VALID_RATIO)\n",
        "\n",
        "    X_train = train.iloc[:split,:-1]\n",
        "    y_train = train.iloc[:split,-1]\n",
        "\n",
        "    X_val = train.iloc[split:,:-1]\n",
        "    y_val = train.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_val,y_val,X_test,y_test = datagen(data,'DJI')\n",
        "reg = xgb.XGBClassifier(objective=\"binary:logistic\")\n",
        "reg.fit(X_train, y_train,\n",
        "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "        early_stopping_rounds=50,\n",
        "       verbose=False)\n",
        "\n",
        "# Test the model\n",
        "test_pred = reg.predict(X_test)\n",
        "#test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvGG9k8NazTH",
        "outputId": "f1d48f95-db13-40b2-a8a8-bf1a17f487ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5365853658536586\n",
            "RMSE: 0.6807456457050177\n",
            "F1: 0.6619217081850534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "NoGJ8dj9c4sv",
        "outputId": "6d63b026-9b96-4142-e57a-1d968eb5e3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6   \\\n",
              "0     1.239175  0.013179 -0.740511 -0.784105  2.109482 -1.498050 -1.062041   \n",
              "1     1.189960  0.094849 -0.869701 -0.732221 -0.792712  2.058288 -0.899532   \n",
              "2     1.204166 -0.797218  0.163581 -0.861941 -0.740016 -0.745595 -0.064367   \n",
              "3     1.081330  0.563787 -2.078129  0.175580 -0.871764 -0.694684 -2.035271   \n",
              "4     1.110822  0.189860  0.422582 -2.075325  0.181982 -0.821969 -1.463069   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "7675  1.091702  0.300852  0.378194 -0.020414  0.370434  0.069613  0.540853   \n",
              "7676  1.031679  0.155009 -0.776163  0.379608 -0.020170  0.384546  0.015456   \n",
              "7677  1.022934 -0.417188 -0.157859 -0.764226  0.385318 -0.001015 -0.085882   \n",
              "7678  0.970895 -0.228240 -0.686514 -0.151558 -0.774146  0.399237 -0.604996   \n",
              "7679  1.003758  0.239569  0.353245 -0.675394 -0.153106 -0.745255 -0.419313   \n",
              "\n",
              "            7         8         9   ...        72        73        74  \\\n",
              "0    -0.624606 -0.092955 -0.504664  ...  0.042195 -0.698095 -0.703821   \n",
              "1    -0.963900 -0.299181 -0.763802  ...  1.406621  0.280642 -1.019295   \n",
              "2    -0.874451 -0.736918 -0.610983  ...  0.648607 -0.240066  0.297809   \n",
              "3    -1.627819 -1.314695 -0.764985  ... -0.576850 -0.741488 -1.744885   \n",
              "4    -1.279408 -1.285538 -0.316214  ...  0.686507 -0.919878 -0.553971   \n",
              "...        ...       ...       ...  ...       ...       ...       ...   \n",
              "7675  0.859542  1.731876  1.970049  ...  1.166583 -0.153281  0.392451   \n",
              "7676  0.341004  1.049817  1.471551  ...  0.016928  0.034752 -0.893105   \n",
              "7677 -0.141991  0.964128  1.518987  ...  0.433836  2.156152 -0.088647   \n",
              "7678 -0.296510  0.791524  1.078837  ...  0.042195 -0.322029 -0.593405   \n",
              "7679 -0.029088  0.231976  0.837812  ...  0.193798  1.514911  0.163732   \n",
              "\n",
              "            75        76        77        78        79        80        81  \n",
              "0    -1.183650 -0.944866 -0.421629 -0.377063 -0.148104 -0.278498  0.406972  \n",
              "1    -0.914834 -1.027191 -2.306612 -2.156426  0.910463  0.013407 -0.099022  \n",
              "2     0.294841  0.548742  1.247367  1.124276  0.627479 -0.132545 -0.117762  \n",
              "3    -2.072813 -0.486199 -0.853604 -0.747763 -0.514934 -0.690292  0.322640  \n",
              "4     0.377553  0.242964 -0.186006  0.049243  0.334015 -0.810181 -1.017307  \n",
              "...        ...       ...       ...       ...       ...       ...       ...  \n",
              "7675 -0.098045  0.266485  0.403052  0.586759  1.775133 -0.221159 -0.277057  \n",
              "7676 -0.180758 -0.345071 -0.382358 -0.339993  1.177724 -0.017868  0.116494  \n",
              "7677  0.129415 -0.545003 -0.382358 -0.321458  0.674643  2.176629  2.149841  \n",
              "7678  0.015685  0.760434  0.913568  0.827715 -0.766475 -0.466151 -0.099022  \n",
              "7679  0.367214 -0.803738 -0.853604 -0.766298 -0.258153  0.206273  0.903596  \n",
              "\n",
              "[7680 rows x 82 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-051da101-a969-485e-80c4-30efa4963740\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.239175</td>\n",
              "      <td>0.013179</td>\n",
              "      <td>-0.740511</td>\n",
              "      <td>-0.784105</td>\n",
              "      <td>2.109482</td>\n",
              "      <td>-1.498050</td>\n",
              "      <td>-1.062041</td>\n",
              "      <td>-0.624606</td>\n",
              "      <td>-0.092955</td>\n",
              "      <td>-0.504664</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042195</td>\n",
              "      <td>-0.698095</td>\n",
              "      <td>-0.703821</td>\n",
              "      <td>-1.183650</td>\n",
              "      <td>-0.944866</td>\n",
              "      <td>-0.421629</td>\n",
              "      <td>-0.377063</td>\n",
              "      <td>-0.148104</td>\n",
              "      <td>-0.278498</td>\n",
              "      <td>0.406972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.189960</td>\n",
              "      <td>0.094849</td>\n",
              "      <td>-0.869701</td>\n",
              "      <td>-0.732221</td>\n",
              "      <td>-0.792712</td>\n",
              "      <td>2.058288</td>\n",
              "      <td>-0.899532</td>\n",
              "      <td>-0.963900</td>\n",
              "      <td>-0.299181</td>\n",
              "      <td>-0.763802</td>\n",
              "      <td>...</td>\n",
              "      <td>1.406621</td>\n",
              "      <td>0.280642</td>\n",
              "      <td>-1.019295</td>\n",
              "      <td>-0.914834</td>\n",
              "      <td>-1.027191</td>\n",
              "      <td>-2.306612</td>\n",
              "      <td>-2.156426</td>\n",
              "      <td>0.910463</td>\n",
              "      <td>0.013407</td>\n",
              "      <td>-0.099022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.204166</td>\n",
              "      <td>-0.797218</td>\n",
              "      <td>0.163581</td>\n",
              "      <td>-0.861941</td>\n",
              "      <td>-0.740016</td>\n",
              "      <td>-0.745595</td>\n",
              "      <td>-0.064367</td>\n",
              "      <td>-0.874451</td>\n",
              "      <td>-0.736918</td>\n",
              "      <td>-0.610983</td>\n",
              "      <td>...</td>\n",
              "      <td>0.648607</td>\n",
              "      <td>-0.240066</td>\n",
              "      <td>0.297809</td>\n",
              "      <td>0.294841</td>\n",
              "      <td>0.548742</td>\n",
              "      <td>1.247367</td>\n",
              "      <td>1.124276</td>\n",
              "      <td>0.627479</td>\n",
              "      <td>-0.132545</td>\n",
              "      <td>-0.117762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.081330</td>\n",
              "      <td>0.563787</td>\n",
              "      <td>-2.078129</td>\n",
              "      <td>0.175580</td>\n",
              "      <td>-0.871764</td>\n",
              "      <td>-0.694684</td>\n",
              "      <td>-2.035271</td>\n",
              "      <td>-1.627819</td>\n",
              "      <td>-1.314695</td>\n",
              "      <td>-0.764985</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.576850</td>\n",
              "      <td>-0.741488</td>\n",
              "      <td>-1.744885</td>\n",
              "      <td>-2.072813</td>\n",
              "      <td>-0.486199</td>\n",
              "      <td>-0.853604</td>\n",
              "      <td>-0.747763</td>\n",
              "      <td>-0.514934</td>\n",
              "      <td>-0.690292</td>\n",
              "      <td>0.322640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.110822</td>\n",
              "      <td>0.189860</td>\n",
              "      <td>0.422582</td>\n",
              "      <td>-2.075325</td>\n",
              "      <td>0.181982</td>\n",
              "      <td>-0.821969</td>\n",
              "      <td>-1.463069</td>\n",
              "      <td>-1.279408</td>\n",
              "      <td>-1.285538</td>\n",
              "      <td>-0.316214</td>\n",
              "      <td>...</td>\n",
              "      <td>0.686507</td>\n",
              "      <td>-0.919878</td>\n",
              "      <td>-0.553971</td>\n",
              "      <td>0.377553</td>\n",
              "      <td>0.242964</td>\n",
              "      <td>-0.186006</td>\n",
              "      <td>0.049243</td>\n",
              "      <td>0.334015</td>\n",
              "      <td>-0.810181</td>\n",
              "      <td>-1.017307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7675</th>\n",
              "      <td>1.091702</td>\n",
              "      <td>0.300852</td>\n",
              "      <td>0.378194</td>\n",
              "      <td>-0.020414</td>\n",
              "      <td>0.370434</td>\n",
              "      <td>0.069613</td>\n",
              "      <td>0.540853</td>\n",
              "      <td>0.859542</td>\n",
              "      <td>1.731876</td>\n",
              "      <td>1.970049</td>\n",
              "      <td>...</td>\n",
              "      <td>1.166583</td>\n",
              "      <td>-0.153281</td>\n",
              "      <td>0.392451</td>\n",
              "      <td>-0.098045</td>\n",
              "      <td>0.266485</td>\n",
              "      <td>0.403052</td>\n",
              "      <td>0.586759</td>\n",
              "      <td>1.775133</td>\n",
              "      <td>-0.221159</td>\n",
              "      <td>-0.277057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7676</th>\n",
              "      <td>1.031679</td>\n",
              "      <td>0.155009</td>\n",
              "      <td>-0.776163</td>\n",
              "      <td>0.379608</td>\n",
              "      <td>-0.020170</td>\n",
              "      <td>0.384546</td>\n",
              "      <td>0.015456</td>\n",
              "      <td>0.341004</td>\n",
              "      <td>1.049817</td>\n",
              "      <td>1.471551</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016928</td>\n",
              "      <td>0.034752</td>\n",
              "      <td>-0.893105</td>\n",
              "      <td>-0.180758</td>\n",
              "      <td>-0.345071</td>\n",
              "      <td>-0.382358</td>\n",
              "      <td>-0.339993</td>\n",
              "      <td>1.177724</td>\n",
              "      <td>-0.017868</td>\n",
              "      <td>0.116494</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7677</th>\n",
              "      <td>1.022934</td>\n",
              "      <td>-0.417188</td>\n",
              "      <td>-0.157859</td>\n",
              "      <td>-0.764226</td>\n",
              "      <td>0.385318</td>\n",
              "      <td>-0.001015</td>\n",
              "      <td>-0.085882</td>\n",
              "      <td>-0.141991</td>\n",
              "      <td>0.964128</td>\n",
              "      <td>1.518987</td>\n",
              "      <td>...</td>\n",
              "      <td>0.433836</td>\n",
              "      <td>2.156152</td>\n",
              "      <td>-0.088647</td>\n",
              "      <td>0.129415</td>\n",
              "      <td>-0.545003</td>\n",
              "      <td>-0.382358</td>\n",
              "      <td>-0.321458</td>\n",
              "      <td>0.674643</td>\n",
              "      <td>2.176629</td>\n",
              "      <td>2.149841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7678</th>\n",
              "      <td>0.970895</td>\n",
              "      <td>-0.228240</td>\n",
              "      <td>-0.686514</td>\n",
              "      <td>-0.151558</td>\n",
              "      <td>-0.774146</td>\n",
              "      <td>0.399237</td>\n",
              "      <td>-0.604996</td>\n",
              "      <td>-0.296510</td>\n",
              "      <td>0.791524</td>\n",
              "      <td>1.078837</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042195</td>\n",
              "      <td>-0.322029</td>\n",
              "      <td>-0.593405</td>\n",
              "      <td>0.015685</td>\n",
              "      <td>0.760434</td>\n",
              "      <td>0.913568</td>\n",
              "      <td>0.827715</td>\n",
              "      <td>-0.766475</td>\n",
              "      <td>-0.466151</td>\n",
              "      <td>-0.099022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7679</th>\n",
              "      <td>1.003758</td>\n",
              "      <td>0.239569</td>\n",
              "      <td>0.353245</td>\n",
              "      <td>-0.675394</td>\n",
              "      <td>-0.153106</td>\n",
              "      <td>-0.745255</td>\n",
              "      <td>-0.419313</td>\n",
              "      <td>-0.029088</td>\n",
              "      <td>0.231976</td>\n",
              "      <td>0.837812</td>\n",
              "      <td>...</td>\n",
              "      <td>0.193798</td>\n",
              "      <td>1.514911</td>\n",
              "      <td>0.163732</td>\n",
              "      <td>0.367214</td>\n",
              "      <td>-0.803738</td>\n",
              "      <td>-0.853604</td>\n",
              "      <td>-0.766298</td>\n",
              "      <td>-0.258153</td>\n",
              "      <td>0.206273</td>\n",
              "      <td>0.903596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7680 rows × 82 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-051da101-a969-485e-80c4-30efa4963740')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-051da101-a969-485e-80c4-30efa4963740 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-051da101-a969-485e-80c4-30efa4963740');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    X_train = df.iloc[:split,:-1]\n",
        "    y_train = df.iloc[:split,-1]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_test,y_test = datagen(data,'DJI')\n",
        "model = SVR(kernel='rbf',gamma=0.5, C=10, epsilon = 0.05)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(X_test)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe44JH_Kc7-n",
        "outputId": "767ab571-e8b3-44fa-be31-f7227f554a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5414634146341464\n",
            "RMSE: 0.6771532953223027\n",
            "F1: 0.7025316455696203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "id": "lpadukgwdRlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ec6129-e7dc-468d-cab5-805723e9aa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['S&P', 'NYA', 'RUT', 'NASDAQ', 'DJI'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    X_train = df.iloc[:split,:-1]\n",
        "    y_train = df.iloc[:split,-1]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_test,y_test = datagen(data,'DJI')\n",
        "model = SVC(kernel='rbf',gamma=0.5, C=10)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Test the model\n",
        "test_pred = model.predict(X_test)\n",
        "#test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "id": "oEvUhTpxexeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ac7b436-7494-48d9-8198-6216badaac00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5414634146341464\n",
            "RMSE: 0.6771532953223027\n",
            "F1: 0.7025316455696203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    train = df.iloc[:split,:]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    split = int(len(index)*TRAIN_VALID_RATIO)\n",
        "\n",
        "    X_train = train.iloc[:split,:-1]\n",
        "    y_train = train.iloc[:split,-1]\n",
        "\n",
        "    X_val = train.iloc[split:,:-1]\n",
        "    y_val = train.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_val,y_val,X_test,y_test= datagen(data,'NYA')\n",
        "error_rate = []\n",
        "# Pick the best n_neighbors\n",
        "for i in range(1,30):\n",
        "  model = KNeighborsClassifier(n_neighbors=i)\n",
        "  model.fit(X_train, y_train)\n",
        "  eval_pred = model.predict(X_val)\n",
        "  error_rate.append(np.mean(eval_pred != y_val))"
      ],
      "metadata": {
        "id": "FLUN89MU9r_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,30),error_rate,color='blue', linestyle='dashed', marker='o',\n",
        " markerfacecolor='red', markersize=10)\n",
        "plt.xticks(np.arange(0,30),np.arange(0,30))\n",
        "plt.title('Error Rate vs. K Value')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Error Rate')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "ibyTPyCyNIuw",
        "outputId": "d56937fa-c4fc-467a-c453-01b1e41dbb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Error Rate')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = KNeighborsClassifier(n_neighbors=28)\n",
        "model.fit(X_train, y_train)\n",
        "test_pred = model.predict(X_test)\n",
        "#test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC2IRe4zO2IF",
        "outputId": "2e340e58-b178-431e-f570-4487154d7b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5414634146341464\n",
            "RMSE: 0.6771532953223027\n",
            "F1: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    train = df.iloc[:split,:]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    split = int(len(index)*TRAIN_VALID_RATIO)\n",
        "\n",
        "    X_train = train.iloc[:split,:-1]\n",
        "    y_train = train.iloc[:split,-1]\n",
        "\n",
        "    X_val = train.iloc[split:,:-1]\n",
        "    y_val = train.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_val,y_val,X_test,y_test = datagen(data,'DJI')\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(5,2),max_iter = 300)\n",
        "mlp_model.fit(X_train, y_train)\n",
        "\n",
        "# Choosing the best hyperparameters\n",
        "# define grid\n",
        "param_grid = { 'hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)],'max_iter': [50, 100, 150],'activation': ['tanh', 'relu'], 'solver': ['sgd', 'adam'],'alpha': [0.0001, 0.05],'learning_rate': ['constant','adaptive']}\n",
        "grid = GridSearchCV(mlp_model, param_grid, n_jobs= -1, cv=5) \n",
        "grid.fit(X_val, y_val)  \n",
        "print(grid.best_params_) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0cpE5xLRpY5",
        "outputId": "c933c09f-d08d-4b6c-e247-cedae0eea426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (120, 80, 40), 'learning_rate': 'constant', 'max_iter': 150, 'solver': 'sgd'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (150) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model = MLPClassifier(hidden_layer_sizes=(120, 80, 40),alpha= 0.05,max_iter = 300,solver= 'sgd')\n",
        "mlp_model.fit(X_train, y_train)\n",
        "# Test the model\n",
        "test_pred = mlp_model.predict(X_test)\n",
        "#test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gsO4LblYXAp",
        "outputId": "1e684a13-b79e-460b-e5af-b3bf33e0dfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5463414634146342\n",
            "RMSE: 0.6735417853298827\n",
            "F1: 0.6804123711340206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    train = df.iloc[:split,:]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    split = int(len(index)*TRAIN_VALID_RATIO)\n",
        "\n",
        "    X_train = train.iloc[:split,:-1]\n",
        "    y_train = train.iloc[:split,-1]\n",
        "\n",
        "    X_val = train.iloc[split:,:-1]\n",
        "    y_val = train.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_val,y_val,X_test,y_test = datagen(data,'DJI')\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Choosing the best hyperparameters\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# define grid\n",
        "param_grid = { 'n_estimators': [25,50,75,100,125,150],'max_features': ['sqrt', 'log2'],'max_depth':max_depth,'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf,'bootstrap':bootstrap}\n",
        "grid = GridSearchCV(model, param_grid, n_jobs= -1, cv=5) \n",
        "grid.fit(X_val, y_val)  \n",
        "print(grid.best_params_) "
      ],
      "metadata": {
        "id": "5cutkExUZ6Ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53939d4a-070e-413c-e631-0404da5002cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bootstrap': True, 'max_depth': 80, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(bootstrap= True, max_depth= 80, max_features= 'log2', min_samples_leaf= 2, min_samples_split= 5, n_estimators= 50)\n",
        "model.fit(X_train, y_train)\n",
        "# Test the model\n",
        "test_pred = model.predict(X_test)\n",
        "#test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnSNkF9hvfBf",
        "outputId": "6ec79e6d-fb81-42d5-8393-32c7397c4cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5024390243902439\n",
            "RMSE: 0.7053800221226542\n",
            "F1: 0.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "#from sklearn.gaussian_process.kernels import DotProduct\n",
        "from sklearn.gaussian_process.kernels import Matern\n",
        "from sklearn.gaussian_process.kernels import RationalQuadratic\n",
        "from sklearn.gaussian_process.kernels import WhiteKernel\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    train = df.iloc[:split,:]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    split = int(len(index)*TRAIN_VALID_RATIO)\n",
        "\n",
        "    X_train = train.iloc[:split,:-1]\n",
        "    y_train = train.iloc[:split,-1]\n",
        "\n",
        "    X_val = train.iloc[split:,:-1]\n",
        "    y_val = train.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_val,y_val,X_test,y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        " \n",
        "\n",
        "# Fit the model\n",
        "X_train,y_train,X_val,y_val,X_test,y_test = datagen(data,'DJI')\n",
        "model = GaussianProcessClassifier()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Choosing the best hyperparameters\n",
        "\n",
        "# define grid\n",
        "grid = dict()\n",
        "grid['kernel'] = [1*RBF(), 1*Matern(),  1*RationalQuadratic(), 1*WhiteKernel()]\n",
        "# define search\n",
        "grid = GridSearchCV(model, grid, scoring='f1', cv=5, n_jobs=-1)\n",
        "# perform the search\n",
        "grid.fit(X_val,y_val)\n",
        "print(grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGGpLwMy3hKA",
        "outputId": "648756d4-a2bc-4230-ae55-c3736ff8fcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'kernel': 1**2 * RBF(length_scale=1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kernel= 1**2 * RBF(length_scale=1)\n",
        "model = GaussianProcessClassifier(kernel = kernel)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "# Test the model\n",
        "test_pred = model.predict(X_test)\n",
        "#test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OkurmaRW5diY",
        "outputId": "a2bb501a-82e5-49ea-d539-c5b0ddc497a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5317073170731708\n",
            "RMSE: 0.6843191382146413\n",
            "F1: 0.5789473684210528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyGRNN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwOL_DQNj_MM",
        "outputId": "dc098a7b-f16a-45de-f174-9078676a0627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyGRNN\n",
            "  Downloading pyGRNN-0.1.2-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGRNN) (1.0.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from pyGRNN) (0.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGRNN) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGRNN) (1.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from pyGRNN) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pyGRNN) (1.3.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyGRNN) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyGRNN) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyGRNN) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->pyGRNN) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->pyGRNN) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->pyGRNN) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pyGRNN) (2022.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyGRNN) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyGRNN) (1.1.0)\n",
            "Installing collected packages: pyGRNN\n",
            "Successfully installed pyGRNN-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyGRNN import GRNN #imports the GRNN regressor module\n",
        "from pyGRNN import feature_selection as FS #imports the GRNN feature selector module\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "\n",
        "\n",
        "def datagen(data,key):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    df = data[key]\n",
        "    index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "    split = len(index)\n",
        "\n",
        "    X_train = df.iloc[:split,:-1]\n",
        "    y_train = df.iloc[:split,-1]\n",
        "\n",
        "    X_test = df.iloc[split:,:-1]\n",
        "    y_test = df.iloc[split:,-1]\n",
        "\n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    #index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "# Get the train_test data\n",
        "X_train,y_train,X_test,y_test = datagen(data,'DJI')\n",
        "feature_names = list(X_train.columns)\n",
        "\n",
        "\n",
        "# Start the process by doing feature selection\n",
        "IsotropicSelector = FS.Isotropic_selector()\n",
        "\n",
        "# Selecting the best subset of features using a forward fs strategy\n",
        "# fs is shortcut for feature selection \n",
        "#IsotropicSelector.ffs(X_train,y_train, feature_names=feature_names)\n",
        "\n",
        "# Selecting the best subset of features using a backward fs strategy\n",
        "#IsotropicSelector.bfs(X_train, y_train, feature_names=feature_names)\n",
        "#IsotropicSelector.relatidness(X_train, feature_names=feature_names, strategy = 'ffs')\n",
        "# Selecting the best subset of features using using an exhaustive search\n",
        "# Exhaustive search : one can go over all possible feature combinations and pick up the model with the highest accuracy.\n",
        "#IsotropicSelector.es(X_train, y_train, feature_names=feature_names)"
      ],
      "metadata": {
        "id": "_Ec2zTTh5t0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Instantiate the estimator\n",
        "IGRNN = GRNN()\n",
        "# Define the parameters for a GridSearch CV and fit the model\n",
        "params_IGRNN = {'kernel':['RBF','Matern',  'RationalQuadratic', 'WhiteKernel','linear', 'poly', 'sigmoid', 'precomputed'],\n",
        "                'sigma' : list(np.arange(0.1, 4, 0.01)),\n",
        "                'calibration' : ['None']\n",
        "                 }\n",
        "grid = GridSearchCV(estimator=IGRNN,\n",
        "                          param_grid=params_IGRNN,\n",
        "                          scoring='neg_mean_squared_error',\n",
        "                          cv=5,\n",
        "                          verbose=False,\n",
        "                          n_jobs = -1\n",
        "                          )\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO_8akrBkCzb",
        "outputId": "d19d267e-bfe1-491d-9ca3-8e2f3d7e90f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [-0.54011293 -0.53901402 -0.52802501 ...         nan         nan\n",
            "         nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=GRNN(), n_jobs=-1,\n",
              "             param_grid={'calibration': ['None'],\n",
              "                         'kernel': ['RBF', 'Matern', 'RationalQuadratic',\n",
              "                                    'WhiteKernel', 'linear', 'poly', 'sigmoid',\n",
              "                                    'precomputed'],\n",
              "                         'sigma': [0.1, 0.11, 0.12, 0.13, 0.13999999999999999,\n",
              "                                   0.14999999999999997, 0.15999999999999998,\n",
              "                                   0.16999999999999998, 0.17999999999999997,\n",
              "                                   0.18999999999999995, 0.19999999999999996,\n",
              "                                   0....\n",
              "                                   0.22999999999999995, 0.23999999999999994,\n",
              "                                   0.24999999999999992, 0.2599999999999999,\n",
              "                                   0.2699999999999999, 0.2799999999999999,\n",
              "                                   0.2899999999999999, 0.29999999999999993,\n",
              "                                   0.30999999999999994, 0.3199999999999999,\n",
              "                                   0.32999999999999985, 0.33999999999999986,\n",
              "                                   0.34999999999999987, 0.3599999999999999,\n",
              "                                   0.3699999999999999, 0.3799999999999999,\n",
              "                                   0.3899999999999999, ...]},\n",
              "             scoring='neg_mean_squared_error', verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = grid.best_estimator_\n",
        "test_out = best_model.predict(X_test)\n",
        "#print(test_pred)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "\n",
        "print(\"accuracy:\", accuracy_score(test_pred, y_test))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(test_pred, y_test)))\n",
        "print(\"F1:\", f1_score(test_pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzy33bJmo6et",
        "outputId": "9c1c00aa-9cb6-4153-8cb1-9cbdcfe1d377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.5121951219512195\n",
            "RMSE: 0.6984302957695782\n",
            "F1: 0.6732026143790849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "from keras import backend as K\n",
        "\n",
        "class RBFLayer(Layer):\n",
        "    def __init__(self, units, gamma, **kwargs):\n",
        "        super(RBFLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.gamma = K.cast_to_floatx(gamma)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "#         print(input_shape)\n",
        "#         print(self.units)\n",
        "        self.mu = self.add_weight(name='mu',\n",
        "                                  shape=(int(input_shape[1]), self.units),\n",
        "                                  initializer='uniform',\n",
        "                                  trainable=True)\n",
        "        super(RBFLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        diff = K.expand_dims(inputs) - self.mu\n",
        "        l2 = K.sum(K.pow(diff, 2), axis=1)\n",
        "        res = K.exp(-1 * self.gamma * l2)\n",
        "        return res\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.units)"
      ],
      "metadata": {
        "id": "TOdVV4-GsABT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.losses import binary_crossentropy\n",
        "\n",
        "DATADIR = \"/content/\"\n",
        "TRAIN_TEST_CUTOFF = '2016-04-21'\n",
        "TRAIN_VALID_RATIO = 0.75\n",
        "\n",
        "# https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "# to implement F1 score for validation in a batch\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\n",
        "def f1macro(y_true, y_pred):\n",
        "    f_pos = f1_m(y_true, y_pred)\n",
        "    # negative version of the data and prediction\n",
        "    f_neg = f1_m(1-y_true, 1-K.clip(y_pred,0,1))\n",
        "    return (f_pos + f_neg)/2\n",
        "\n",
        "def rbf_model(seq_len=60, n_features=82, n_filters=(8,8,8), droprate=0.1):\n",
        "    \"2D-CNNpred model according to the paper\"\n",
        "    model = Sequential([\n",
        "        Input(shape=(seq_len, n_features, 1)),\n",
        "        Flatten(),\n",
        "        RBFLayer(10, 0.5),\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def datagen(data, seq_len, batch_size, targetcol, kind):\n",
        "    \"As a generator to produce samples for Keras model\"\n",
        "    batch = []\n",
        "    while True:\n",
        "        # Pick one dataframe from the pool\n",
        "        key = random.choice(list(data.keys()))\n",
        "        df = data[key]\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        index = df.index[df.index < TRAIN_TEST_CUTOFF]\n",
        "        split = int(len(index) * TRAIN_VALID_RATIO)\n",
        "        assert split > seq_len, \"Training data too small for sequence length {}\".format(seq_len)\n",
        "        if kind == 'train':\n",
        "            index = index[:split]   # range for the training set\n",
        "        elif kind == 'valid':\n",
        "            index = index[split:]   # range for the validation set\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # Pick one position, then clip a sequence length\n",
        "        while True:\n",
        "            t = random.choice(index)     # pick one time step\n",
        "            n = (df.index == t).argmax() # find its position in the dataframe\n",
        "            if n-seq_len+1 < 0:\n",
        "                continue # this sample is not enough for one sequence length\n",
        "            frame = df.iloc[n-seq_len+1:n+1]\n",
        "            batch.append([frame[input_cols].values, df.loc[t, targetcol]])\n",
        "            break\n",
        "        # if we get enough for a batch, dispatch\n",
        "        if len(batch) == batch_size:\n",
        "            X, y = zip(*batch)\n",
        "            X, y = np.expand_dims(np.array(X), 3), np.array(y)\n",
        "            yield X, y\n",
        "            batch = []\n",
        "\n",
        "def testgen(data, seq_len, targetcol):\n",
        "    \"Return array of all test samples\"\n",
        "    batch = []\n",
        "    for key, df in data.items():\n",
        "        input_cols = [c for c in df.columns if c != targetcol]\n",
        "        # find the start of test sample\n",
        "        t = df.index[df.index >= TRAIN_TEST_CUTOFF][0]\n",
        "        n = (df.index == t).argmax()\n",
        "        # extract sample using a sliding window\n",
        "        for i in range(n+1, len(df)+1):\n",
        "            frame = df.iloc[i-seq_len:i]\n",
        "            batch.append([frame[input_cols].values, frame[targetcol][-1]])\n",
        "    X, y = zip(*batch)\n",
        "    return np.expand_dims(np.array(X),3), np.array(y)\n",
        "\n",
        "# Read data into pandas DataFrames\n",
        "data = {}\n",
        "for filename in os.listdir('/content/'):\n",
        "    if not filename.lower().endswith(\".csv\"):\n",
        "        continue # read only the CSV files\n",
        "    filepath = os.path.join('/content/', filename)\n",
        "    X = pd.read_csv(filepath, index_col=\"Date\", parse_dates=True)\n",
        "    # basic preprocessing: get the name, the classification\n",
        "    # Save the target variable as a column in dataframe for easier dropna()\n",
        "    name = X[\"Name\"][0]\n",
        "    del X[\"Name\"]\n",
        "    cols = X.columns\n",
        "    X[\"Target\"] = (X[\"Close\"].pct_change().shift(-1) > 0).astype(int)\n",
        "    X.dropna(inplace=True)\n",
        "    # Fit the standard scaler using the training dataset\n",
        "    index = X.index[X.index < TRAIN_TEST_CUTOFF]\n",
        "    index = index[:int(len(index) * TRAIN_VALID_RATIO)]\n",
        "    scaler = StandardScaler().fit(X.loc[index, cols])\n",
        "    # Save scale transformed dataframe\n",
        "    X[cols] = scaler.transform(X[cols])\n",
        "    data[name] = X\n",
        "\n",
        "seq_len = 60\n",
        "batch_size = 128\n",
        "n_epochs = 8\n",
        "n_features = 82\n",
        "\n",
        "# Produce CNNpred as a binary classification problem\n",
        "model = rbf_model(seq_len, n_features)\n",
        "model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[\"acc\", f1macro])\n",
        "model.summary()  # print model structure to console\n",
        "\n",
        "# Set up callbacks and fit the model\n",
        "# We use custom validation score f1macro() and hence monitor for \"val_f1macro\"\n",
        "checkpoint_path = \"./cp2d-{epoch}-{val_f1macro:.2f}.h5\"\n",
        "\"\"\"callbacks = [\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                    monitor='val_f1macro', mode=\"max\",\n",
        "                    verbose=0, save_best_only=True, save_weights_only=False, save_freq=\"epoch\")\n",
        "]\"\"\"\n",
        "model.fit(datagen(data, seq_len, batch_size, \"Target\", \"train\"),\n",
        "          validation_data=datagen(data, seq_len, batch_size, \"Target\", \"valid\"),\n",
        "          epochs=n_epochs, steps_per_epoch=400, validation_steps=10, verbose=1)\n",
        "\n",
        "# Prepare test data\n",
        "test_data, test_target = testgen(data, seq_len, \"Target\")\n",
        "\n",
        "# Test the model\n",
        "test_out = model.predict(test_data)\n",
        "test_pred = (test_out > 0.5).astype(int)\n",
        "print(\"accuracy:\", accuracy_score(test_pred, test_target))\n",
        "print(\"MAE:\", mean_absolute_error(test_pred, test_target))\n",
        "print(\"F1:\", f1_score(test_pred, test_target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh5gmfZnt-9G",
        "outputId": "93a6f4c8-7b87-4b6e-82a3-82ecce9dd945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 4920)              0         \n",
            "                                                                 \n",
            " rbf_layer_2 (RBFLayer)      (None, 10)                49200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 49,211\n",
            "Trainable params: 49,211\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/8\n",
            "400/400 [==============================] - 47s 116ms/step - loss: 0.4949 - acc: 0.5593 - f1macro: 0.3574 - val_loss: 0.4948 - val_acc: 0.5312 - val_f1macro: 0.3463\n",
            "Epoch 2/8\n",
            "400/400 [==============================] - 46s 116ms/step - loss: 0.4864 - acc: 0.5571 - f1macro: 0.3573 - val_loss: 0.4911 - val_acc: 0.5289 - val_f1macro: 0.3456\n",
            "Epoch 3/8\n",
            "400/400 [==============================] - 47s 117ms/step - loss: 0.4779 - acc: 0.5592 - f1macro: 0.3582 - val_loss: 0.5075 - val_acc: 0.4828 - val_f1macro: 0.3251\n",
            "Epoch 4/8\n",
            "400/400 [==============================] - 46s 116ms/step - loss: 0.4710 - acc: 0.5592 - f1macro: 0.3581 - val_loss: 0.4970 - val_acc: 0.5055 - val_f1macro: 0.3351\n",
            "Epoch 5/8\n",
            "400/400 [==============================] - 46s 116ms/step - loss: 0.4664 - acc: 0.5576 - f1macro: 0.3574 - val_loss: 0.4868 - val_acc: 0.5211 - val_f1macro: 0.3417\n",
            "Epoch 6/8\n",
            "400/400 [==============================] - 46s 116ms/step - loss: 0.4603 - acc: 0.5601 - f1macro: 0.3585 - val_loss: 0.4940 - val_acc: 0.5086 - val_f1macro: 0.3365\n",
            "Epoch 7/8\n",
            "400/400 [==============================] - 47s 117ms/step - loss: 0.4569 - acc: 0.5597 - f1macro: 0.3584 - val_loss: 0.4965 - val_acc: 0.5047 - val_f1macro: 0.3351\n",
            "Epoch 8/8\n",
            "400/400 [==============================] - 46s 115ms/step - loss: 0.4522 - acc: 0.5618 - f1macro: 0.3592 - val_loss: 0.4950 - val_acc: 0.5063 - val_f1macro: 0.3358\n",
            "accuracy: 0.5375609756097561\n",
            "MAE: 0.4624390243902439\n",
            "F1: 0.6992385786802031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FntzxK7H1-sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VY4WPRapqK1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOMUUDioqRGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zoIIaC8Qqkzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hQjx9mIrWU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KoJZI6h2rc26"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}